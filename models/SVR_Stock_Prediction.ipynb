{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c763c75",
   "metadata": {},
   "source": [
    "# SVR Stock Price Prediction Model\n",
    "\n",
    "SVR Stock Price Prediction Model\n",
    "Author: Ashutosh Talekar\n",
    "Course: ISE 464/364 - Coding Project\n",
    "Task: Support Vector Regression for Stock Price Prediction\n",
    "\n",
    "This implementation covers:\n",
    "1. Problem Formulation & Motivation\n",
    "2. Data Acquisition & Preparation\n",
    "3. Data Exploratory Analysis\n",
    "4. Model Selection & Training (SVR)\n",
    "5. Evaluation & Analysis\n",
    "6. Communication & Presentation\n",
    "\n",
    "Advanced contributions include:\n",
    "- Financial feature engineering (technical indicators)\n",
    "- Thorough data cleaning with clear rationale\n",
    "- Multiple SVR kernel comparisons\n",
    "- Comprehensive model evaluation with confidence intervals\n",
    "- Analysis of model assumptions and error patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01829548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc0059",
   "metadata": {},
   "source": [
    "## 1. Problem Formulation & Motivation\n",
    "\n",
    "**Problem Statement:**\n",
    "Predict weekly stock returns for Apple Inc. (AAPL) using Support Vector Regression.\n",
    "\n",
    "**Inputs:**\n",
    "- Historical OHLCV data (Open, High, Low, Close, Volume)\n",
    "- Engineered technical indicators\n",
    "- Temporal features\n",
    "\n",
    "**Target:**\n",
    "- Weekly forward returns (% change in closing price)\n",
    "\n",
    "**Why SVR?**\n",
    "1. Captures non-linear relationships\n",
    "2. Robust to outliers\n",
    "3. Kernel trick for complex patterns\n",
    "4. Regularization prevents overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SVR STOCK PRICE PREDICTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. PROBLEM FORMULATION & MOTIVATION\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\"\"\n",
    "PROBLEM STATEMENT:\n",
    "Predict weekly stock returns for Apple Inc. (AAPL) using Support Vector Regression.\n",
    "This enables data-driven portfolio optimization and investment decision-making.\n",
    "\n",
    "INPUTS:\n",
    "- Historical OHLCV data (Open, High, Low, Close, Volume)\n",
    "- Engineered technical indicators (moving averages, volatility, momentum)\n",
    "- Temporal features (day of week, month)\n",
    "\n",
    "TARGETS:\n",
    "- Weekly forward returns (% change in closing price)\n",
    "\n",
    "MOTIVATION:\n",
    "Stock price prediction is crucial for portfolio management and risk assessment.\n",
    "SVR is particularly well-suited for this task because:\n",
    "1. Captures non-linear relationships in financial data\n",
    "2. Robust to outliers through epsilon-insensitive loss\n",
    "3. Kernel trick enables complex pattern recognition\n",
    "4. Regularization prevents overfitting in noisy financial data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b6e5ef",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db1310",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. DATA ACQUISITION & PREPARATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load raw data\n",
    "print(\"Loading data from Yahoo Finance CSV...\")\n",
    "raw = pd.read_csv(\"AAPL_since_IPO_OHLCV.csv\", header=None)\n",
    "\n",
    "print(f\"Raw data shape: {raw.shape}\")\n",
    "print(f\"First few rows:\\n{raw.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning with Clear Rationale\n",
    "print(\"\\nData Cleaning Steps:\")\n",
    "print(\"1. Removing metadata rows (rows 0-2 contain headers/ticker info)\")\n",
    "raw = raw.drop(index=[0, 1, 2]).reset_index(drop=True)\n",
    "\n",
    "print(\"2. Setting proper column names\")\n",
    "raw.columns = [\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "\n",
    "print(\"3. Converting Date column to datetime format\")\n",
    "raw[\"Date\"] = pd.to_datetime(raw[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "print(\"4. Converting numeric columns from string to float\")\n",
    "num_cols = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "raw[num_cols] = raw[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "print(f\"5. Handling missing values - Found {raw.isnull().sum().sum()} missing values\")\n",
    "raw = raw.dropna().sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "print(f\"6. Removing duplicate dates\")\n",
    "initial_count = len(raw)\n",
    "raw = raw.drop_duplicates(subset=['Date'], keep='first')\n",
    "print(f\"   Removed {initial_count - len(raw)} duplicate entries\")\n",
    "\n",
    "print(f\"\\nCleaned data shape: {raw.shape}\")\n",
    "print(f\"Date range: {raw['Date'].min()} to {raw['Date'].max()}\")\n",
    "print(f\"Total trading days: {len(raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b4d30",
   "metadata": {},
   "source": [
    "## 3. Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n3. DATA EXPLORATORY ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(raw[num_cols].describe())\n",
    "\n",
    "# Focus on recent data\n",
    "print(\"\\nFocusing on last 10 years for relevant market conditions...\")\n",
    "recent_cutoff = raw['Date'].max() - pd.Timedelta(days=365*10)\n",
    "df = raw[raw['Date'] >= recent_cutoff].copy()\n",
    "df = df.set_index('Date')\n",
    "\n",
    "print(f\"Working dataset: {len(df)} observations from {df.index.min()} to {df.index.max()}\")\n",
    "\n",
    "# Calculate returns\n",
    "print(\"\\nCalculating returns...\")\n",
    "df['Daily_Return'] = df['Close'].pct_change()\n",
    "df['Weekly_Return'] = df['Close'].pct_change(periods=5)  # Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcc902",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Creating comprehensive technical indicators:\n",
    "- Moving Averages (SMA, EMA)\n",
    "- Volatility metrics\n",
    "- Momentum indicators\n",
    "- RSI, MACD\n",
    "- Bollinger Bands\n",
    "- Volume indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Technical Indicators\n",
    "print(\"\\nEngineering Technical Indicators:\")\n",
    "print(\"- Moving Averages (SMA 5, 20, 50)\")\n",
    "df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
    "\n",
    "print(\"- Exponential Moving Averages (EMA 12, 26)\")\n",
    "df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "print(\"- Volatility (20-day rolling standard deviation)\")\n",
    "df['Volatility'] = df['Daily_Return'].rolling(window=20).std()\n",
    "\n",
    "print(\"- Price momentum indicators\")\n",
    "df['Momentum_5'] = df['Close'] - df['Close'].shift(5)\n",
    "df['Momentum_20'] = df['Close'] - df['Close'].shift(20)\n",
    "\n",
    "print(\"- Relative Strength Index (RSI)\")\n",
    "delta = df['Close'].diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "rs = gain / loss\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "print(\"- MACD (Moving Average Convergence Divergence)\")\n",
    "df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "\n",
    "print(\"- Bollinger Bands\")\n",
    "df['BB_middle'] = df['Close'].rolling(window=20).mean()\n",
    "df['BB_std'] = df['Close'].rolling(window=20).std()\n",
    "df['BB_upper'] = df['BB_middle'] + (df['BB_std'] * 2)\n",
    "df['BB_lower'] = df['BB_middle'] - (df['BB_std'] * 2)\n",
    "df['BB_width'] = df['BB_upper'] - df['BB_lower']\n",
    "\n",
    "print(\"- Volume indicators\")\n",
    "df['Volume_MA_20'] = df['Volume'].rolling(window=20).mean()\n",
    "df['Volume_Ratio'] = df['Volume'] / df['Volume_MA_20']\n",
    "\n",
    "print(\"- Price range indicators\")\n",
    "df['High_Low_Range'] = df['High'] - df['Low']\n",
    "df['Close_Open_Range'] = df['Close'] - df['Open']\n",
    "\n",
    "# Lag features\n",
    "print(\"- Creating lag features (previous returns)\")\n",
    "for lag in [1, 2, 3, 5, 10]:\n",
    "    df[f'Return_Lag_{lag}'] = df['Daily_Return'].shift(lag)\n",
    "\n",
    "# Temporal features\n",
    "print(\"- Temporal features (day of week, month)\")\n",
    "df['DayOfWeek'] = df.index.dayofweek\n",
    "df['Month'] = df.index.month\n",
    "\n",
    "# Drop NaN values\n",
    "print(f\"\\nDropping rows with NaN values from feature engineering...\")\n",
    "print(f\"Before: {len(df)} rows\")\n",
    "df = df.dropna()\n",
    "print(f\"After: {len(df)} rows\")\n",
    "\n",
    "print(f\"\\nFinal dataset ready with {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aedb29",
   "metadata": {},
   "source": [
    "## 4. Model Selection & Training\n",
    "\n",
    "**Why Support Vector Regression?**\n",
    "\n",
    "1. **Non-linear relationships:** Financial data exhibits complex patterns\n",
    "2. **Robustness:** Epsilon-insensitive loss handles outliers\n",
    "3. **Kernel trick:** Can model complex decision boundaries\n",
    "4. **Regularization:** Prevents overfitting in noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af89882",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4. MODEL SELECTION & TRAINING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "WHY SUPPORT VECTOR REGRESSION (SVR)?\n",
    "\n",
    "SVR is chosen for stock price prediction because:\n",
    "\n",
    "1. NON-LINEAR RELATIONSHIPS: Financial data exhibits complex non-linear patterns\n",
    "   that linear models cannot capture. SVR with RBF kernel can model these patterns.\n",
    "\n",
    "2. ROBUSTNESS TO OUTLIERS: The epsilon-insensitive loss function means SVR\n",
    "   is not heavily influenced by extreme price movements or market shocks.\n",
    "\n",
    "3. KERNEL TRICK: Allows mapping to higher dimensions without explicit computation,\n",
    "   enabling capture of complex patterns in the feature space.\n",
    "\n",
    "4. REGULARIZATION: C parameter controls trade-off between margin maximization\n",
    "   and training error, preventing overfitting in noisy financial data.\n",
    "\n",
    "5. EFFICIENCY: Sparse solution depends only on support vectors, not all data points.\n",
    "\"\"\")\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = [col for col in df.columns if col not in ['Weekly_Return', 'Close', 'High', 'Low', 'Open', 'Volume']]\n",
    "X = df[feature_columns]\n",
    "y = df['Weekly_Return']\n",
    "\n",
    "print(f\"\\nFeatures: {len(feature_columns)} total\")\n",
    "print(f\"Target: Weekly_Return\")\n",
    "print(f\"Dataset size: {len(X)} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb927c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split with time series consideration\n",
    "print(\"\\nSplitting data (80% train, 20% test)...\")\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training set: {len(X_train)} observations ({X_train.index.min()} to {X_train.index.max()})\")\n",
    "print(f\"Test set: {len(X_test)} observations ({X_test.index.min()} to {X_test.index.max()})\")\n",
    "\n",
    "# Standardization\n",
    "print(\"\\nStandardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"✓ Features scaled to zero mean and unit variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec9dad",
   "metadata": {},
   "source": [
    "## Training Multiple SVR Kernels\n",
    "\n",
    "Comparing Linear, Polynomial, and RBF kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ae7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple SVR models with different kernels\n",
    "print(\"\\nTraining SVR models with different kernels...\")\n",
    "models = {\n",
    "    'Linear': SVR(kernel='linear', C=1.0, epsilon=0.01),\n",
    "    'Polynomial': SVR(kernel='poly', C=1.0, degree=2, epsilon=0.01),\n",
    "    'RBF': SVR(kernel='rbf', C=1.0, gamma='scale', epsilon=0.01)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name} Kernel SVR...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    cv_scores = -cross_val_score(model, X_train_scaled, y_train, \n",
    "                                  cv=tscv, scoring='neg_root_mean_squared_error')\n",
    "    cv_rmse = cv_scores.mean()\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'support_vectors': len(model.support_)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name} Kernel Results:\")\n",
    "    print(f\"  Training RMSE: {train_rmse:.6f}\")\n",
    "    print(f\"  Test RMSE: {test_rmse:.6f}\")\n",
    "    print(f\"  Training MAE: {train_mae:.6f}\")\n",
    "    print(f\"  Test MAE: {test_mae:.6f}\")\n",
    "    print(f\"  Training R²: {train_r2:.4f}\")\n",
    "    print(f\"  Test R²: {test_r2:.4f}\")\n",
    "    print(f\"  CV RMSE: {cv_rmse:.6f} (±{cv_scores.std():.6f})\")\n",
    "    print(f\"  Support Vectors: {len(model.support_)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model Training Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Kernel': list(results.keys()),\n",
    "    'Test_RMSE': [results[k]['test_rmse'] for k in results.keys()],\n",
    "    'Test_MAE': [results[k]['test_mae'] for k in results.keys()],\n",
    "    'Test_R2': [results[k]['test_r2'] for k in results.keys()],\n",
    "    'CV_RMSE': [results[k]['cv_rmse'] for k in results.keys()],\n",
    "    'Support_Vectors': [results[k]['support_vectors'] for k in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_model_name = comparison_df.loc[comparison_df['Test_RMSE'].idxmin(), 'Kernel']\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"\\n✓ Best Model: {best_model_name} Kernel (Lowest Test RMSE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722a06dc",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0affa6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n5. MODEL EVALUATION & ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Detailed evaluation of best model\n",
    "y_pred_test = results[best_model_name]['y_pred_test']\n",
    "residuals = y_test - y_pred_test\n",
    "\n",
    "# Directional accuracy\n",
    "directional_accuracy = np.mean((y_test > 0) == (y_pred_test > 0))\n",
    "print(f\"\\nDirectional Accuracy: {directional_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confidence intervals (bootstrap)\n",
    "n_bootstrap = 1000\n",
    "bootstrap_rmse = []\n",
    "for _ in range(n_bootstrap):\n",
    "    idx = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    bootstrap_rmse.append(np.sqrt(mean_squared_error(y_test.iloc[idx], y_pred_test[idx])))\n",
    "\n",
    "ci_lower, ci_upper = np.percentile(bootstrap_rmse, [2.5, 97.5])\n",
    "print(f\"\\n95% Confidence Interval for RMSE: [{ci_lower:.6f}, {ci_upper:.6f}]\")\n",
    "\n",
    "# Residual analysis\n",
    "print(f\"\\nResidual Analysis:\")\n",
    "print(f\"  Mean: {residuals.mean():.6f}\")\n",
    "print(f\"  Std Dev: {residuals.std():.6f}\")\n",
    "print(f\"  Skewness: {stats.skew(residuals):.4f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(residuals):.4f}\")\n",
    "\n",
    "# Normality test\n",
    "statistic, p_value = stats.shapiro(residuals[:5000] if len(residuals) > 5000 else residuals)\n",
    "print(f\"\\nShapiro-Wilk Test for Normality:\")\n",
    "print(f\"  Statistic: {statistic:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  {'Residuals are approximately normal' if p_value > 0.05 else 'Residuals deviate from normality'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a55c305",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n6. VISUALIZATIONS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create comprehensive visualization dashboard\n",
    "fig = plt.figure(figsize=(20, 24))\n",
    "gs = fig.add_gridspec(6, 2, hspace=0.3, wspace=0.25)\n",
    "\n",
    "# 1. Actual vs Predicted (Training)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.scatter(results[best_model_name]['y_pred_train'], y_train, alpha=0.3, s=20)\n",
    "ax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "ax1.set_title(f'{best_model_name} Kernel - Training Set', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Weekly Return')\n",
    "ax1.set_ylabel('Actual Weekly Return')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.text(0.05, 0.95, f'R² = {results[best_model_name][\"train_r2\"]:.4f}', \n",
    "         transform=ax1.transAxes, fontsize=11, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 2. Actual vs Predicted (Test)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.scatter(y_pred_test, y_test, alpha=0.5, s=30, c='green')\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax2.set_title(f'{best_model_name} Kernel - Test Set', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Predicted Weekly Return')\n",
    "ax2.set_ylabel('Actual Weekly Return')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.text(0.05, 0.95, f'R² = {results[best_model_name][\"test_r2\"]:.4f}\\nRMSE = {results[best_model_name][\"test_rmse\"]:.6f}', \n",
    "         transform=ax2.transAxes, fontsize=11, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# 3. Model Comparison\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "ax3.bar(x_pos, comparison_df['Test_RMSE'], color=['red' if k == best_model_name else 'gray' for k in comparison_df['Kernel']])\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(comparison_df['Kernel'])\n",
    "ax3.set_ylabel('Test RMSE')\n",
    "ax3.set_title('Model Comparison - Test RMSE', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. R² Comparison\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax4.bar(x_pos, comparison_df['Test_R2'], color=['green' if k == best_model_name else 'gray' for k in comparison_df['Kernel']])\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(comparison_df['Kernel'])\n",
    "ax4.set_ylabel('Test R²')\n",
    "ax4.set_title('Model Comparison - Test R²', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Time Series of Predictions\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "test_dates = y_test.index\n",
    "ax5.plot(test_dates, y_test.values, label='Actual', linewidth=1.5, alpha=0.7)\n",
    "ax5.plot(test_dates, y_pred_test, label='Predicted', linewidth=1.5, alpha=0.7)\n",
    "ax5.fill_between(test_dates, y_test.values, y_pred_test, alpha=0.2)\n",
    "ax5.set_title('Time Series: Actual vs Predicted Weekly Returns', fontsize=12, fontweight='bold')\n",
    "ax5.set_xlabel('Date')\n",
    "ax5.set_ylabel('Weekly Return')\n",
    "ax5.legend(loc='upper right')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Error Distribution by Time\n",
    "ax6 = fig.add_subplot(gs[3, :])\n",
    "ax6.scatter(test_dates, residuals, alpha=0.5, s=20)\n",
    "ax6.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax6.fill_between(test_dates, -2*residuals.std(), 2*residuals.std(), alpha=0.2, color='gray')\n",
    "ax6.set_title('Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "ax6.set_xlabel('Date')\n",
    "ax6.set_ylabel('Residual (Actual - Predicted)')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Cumulative Returns\n",
    "ax7 = fig.add_subplot(gs[4, 0])\n",
    "actual_cumulative = (1 + y_test).cumprod()\n",
    "predicted_cumulative = (1 + pd.Series(y_pred_test, index=y_test.index)).cumprod()\n",
    "ax7.plot(actual_cumulative.index, actual_cumulative.values, label='Actual', linewidth=2)\n",
    "ax7.plot(predicted_cumulative.index, predicted_cumulative.values, label='Predicted', linewidth=2, alpha=0.7)\n",
    "ax7.set_title('Cumulative Returns Comparison', fontsize=12, fontweight='bold')\n",
    "ax7.set_xlabel('Date')\n",
    "ax7.set_ylabel('Cumulative Return')\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Residual Plot\n",
    "ax8 = fig.add_subplot(gs[4, 1])\n",
    "ax8.scatter(y_pred_test, residuals, alpha=0.5, s=30)\n",
    "ax8.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax8.set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "ax8.set_xlabel('Predicted Weekly Return')\n",
    "ax8.set_ylabel('Residuals')\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Residual Distribution\n",
    "ax9 = fig.add_subplot(gs[5, 0])\n",
    "ax9.hist(residuals, bins=30, edgecolor='black', alpha=0.7, density=True)\n",
    "mu, std = residuals.mean(), residuals.std()\n",
    "x = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "ax9.plot(x, stats.norm.pdf(x, mu, std), 'r-', linewidth=2, label='Normal Distribution')\n",
    "ax9.set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
    "ax9.set_xlabel('Residual')\n",
    "ax9.set_ylabel('Density')\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "# 10. Q-Q Plot\n",
    "ax10 = fig.add_subplot(gs[5, 1])\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax10)\n",
    "ax10.set_title('Q-Q Plot of Residuals', fontsize=12, fontweight='bold')\n",
    "ax10.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('SVR Stock Price Prediction - Comprehensive Analysis', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig('SVR_Analysis_Dashboard.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Comprehensive dashboard saved: SVR_Analysis_Dashboard.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6459ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical Indicators Visualization\n",
    "fig2, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "fig2.suptitle('Technical Indicators Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# RSI\n",
    "axes[0, 0].plot(df.index, df['RSI'], linewidth=1)\n",
    "axes[0, 0].axhline(y=70, color='r', linestyle='--', alpha=0.5, label='Overbought')\n",
    "axes[0, 0].axhline(y=30, color='g', linestyle='--', alpha=0.5, label='Oversold')\n",
    "axes[0, 0].set_title('Relative Strength Index (RSI)')\n",
    "axes[0, 0].set_ylabel('RSI')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MACD\n",
    "axes[0, 1].plot(df.index, df['MACD'], label='MACD', linewidth=1)\n",
    "axes[0, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "axes[0, 1].set_title('MACD')\n",
    "axes[0, 1].set_ylabel('MACD')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Bollinger Bands\n",
    "axes[1, 0].plot(df.index, df['Close'], label='Close', linewidth=1)\n",
    "axes[1, 0].plot(df.index, df['BB_upper'], label='Upper Band', alpha=0.7, linewidth=1)\n",
    "axes[1, 0].plot(df.index, df['BB_lower'], label='Lower Band', alpha=0.7, linewidth=1)\n",
    "axes[1, 0].fill_between(df.index, df['BB_lower'], df['BB_upper'], alpha=0.2)\n",
    "axes[1, 0].set_title('Bollinger Bands')\n",
    "axes[1, 0].set_ylabel('Price ($)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume\n",
    "axes[1, 1].bar(df.index, df['Volume'], alpha=0.5, width=1)\n",
    "axes[1, 1].plot(df.index, df['Volume_MA_20'], color='red', label='20-Day MA', linewidth=2)\n",
    "axes[1, 1].set_title('Trading Volume')\n",
    "axes[1, 1].set_ylabel('Volume')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Momentum\n",
    "axes[2, 0].plot(df.index, df['Momentum_5'], label='5-Day', linewidth=1)\n",
    "axes[2, 0].plot(df.index, df['Momentum_20'], label='20-Day', linewidth=1, alpha=0.7)\n",
    "axes[2, 0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "axes[2, 0].set_title('Price Momentum')\n",
    "axes[2, 0].set_ylabel('Momentum ($)')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature Correlation\n",
    "correlations = X.corrwith(y).sort_values(ascending=False)\n",
    "top_10 = correlations.head(10)\n",
    "y_pos = np.arange(len(top_10))\n",
    "axes[2, 1].barh(y_pos, top_10.values)\n",
    "axes[2, 1].set_yticks(y_pos)\n",
    "axes[2, 1].set_yticklabels(top_10.index, fontsize=8)\n",
    "axes[2, 1].set_xlabel('Correlation')\n",
    "axes[2, 1].set_title('Top 10 Features')\n",
    "axes[2, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Technical_Indicators_Analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Technical indicators saved: Technical_Indicators_Analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47abf6b1",
   "metadata": {},
   "source": [
    "## 7. Summary & Key Insights\n",
    "\n",
    "### Model Performance\n",
    "- Best performing kernel selected based on test RMSE\n",
    "- Comprehensive evaluation with multiple metrics\n",
    "- Cross-validation ensures robustness\n",
    "\n",
    "### Key Findings\n",
    "1. **Model Selection:** Kernel comparison reveals optimal approach\n",
    "2. **Predictive Power:** R² and RMSE indicate prediction quality\n",
    "3. **Directional Accuracy:** Critical for trading strategies\n",
    "4. **Feature Importance:** Technical indicators contribute significantly\n",
    "\n",
    "### Practical Applications\n",
    "- Weekly portfolio rebalancing\n",
    "- Risk assessment through prediction intervals\n",
    "- Combined with fundamental analysis for decisions\n",
    "\n",
    "### Limitations\n",
    "- Past performance ≠ future results\n",
    "- Market regime changes affect accuracy\n",
    "- Black swan events not captured\n",
    "- One input among many in investment decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb2b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY & KEY INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "MODEL PERFORMANCE SUMMARY:\n",
    "- Best Model: {best_model_name} Kernel SVR\n",
    "- Test RMSE: {results[best_model_name]['test_rmse']:.6f}\n",
    "- Test MAE: {results[best_model_name]['test_mae']:.6f}\n",
    "- Test R²: {results[best_model_name]['test_r2']:.4f}\n",
    "- Directional Accuracy: {directional_accuracy * 100:.2f}%\n",
    "- Cross-Validation RMSE: {results[best_model_name]['cv_rmse']:.6f}\n",
    "\n",
    "KEY INSIGHTS:\n",
    "1. Model Selection: {best_model_name} kernel performed best\n",
    "2. R² of {results[best_model_name]['test_r2']:.4f} indicates {\"strong\" if results[best_model_name]['test_r2'] > 0.3 else \"modest\"} predictive power\n",
    "3. Directional accuracy: {directional_accuracy * 100:.1f}% \n",
    "4. Technical indicators provide valuable signal\n",
    "5. Suitable for portfolio optimization with proper risk management\n",
    "\n",
    "FUTURE ENHANCEMENTS:\n",
    "- Hyperparameter tuning (grid search)\n",
    "- Ensemble methods\n",
    "- Macroeconomic indicators\n",
    "- Sentiment analysis\n",
    "- Multi-output prediction\n",
    "- Online learning\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78556ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and results\n",
    "import pickle\n",
    "\n",
    "with open('best_svr_model.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': best_model,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_columns,\n",
    "        'results': results,\n",
    "        'comparison_df': comparison_df\n",
    "    }, f)\n",
    "\n",
    "print(\"✓ Model and results saved to: best_svr_model.pkl\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
